---
title: 煎蛋网图片下载
date: 2018-05-08 18:40:45
categories: 
- 技术
- 爬虫
tags: 爬虫
copyright: true
comments: true
reward: true
---

# 概述

看到篇介绍，重点在加密链接的破解。由于暂时没找到类似的加密方法，无从练习，就重新回顾下煎蛋网破解和下载的过程。

# 前期分析

用Chrome检查图片，复制`CSS-Selector:#comment-3794125 > div > div > div.text > p > img`，从#可知该图片是由js加载的。
<!-- more -->
# 获得图片链接

进一步分析网页的源代码，查找相应的ID可以找到

```html
<div class="text"><span class="righttext"><a href="//jandan.net/ooxx/page-64#comment-3794125">3794125</a></span><p><img src="//img.jandan.net/img/blank.gif" onload="jandan_load_img(this)" /><span class="img-hash">c683JAKzbdNd11FQyKWhQuMeBjpsX+5z5ydD/Q1fCvWWHYGkBOzgcFsLY77r+F/bOowKq8sScGJdObL41G5rBo00WMavV48AA6pvqaE6HPWZs8X1hkUuzw</span></p>
```

分析可知 网片是由`jandan_load_img(this)`函数加载的，区别码是`img-hash`下的文本，在相应的js代码中搜索函数，最终在链接

```html
<script src="//cdn.jandan.net/static/min/d5ede23626d328724a5765b23e13b6f7efdnwMLG.15100001.js"></script>
```

找到函数定义

```js
function jandan_load_img(b)
{   var d=$(b);
    var f=d.next("span.img-hash");
    var e=f.text();
    f.remove();
    var c=jdM4UbnT0aS3FCLFAblepJspDWH70CjuG6(e,"UCghC5vslIL5v1wrIBeVnMlD51DQTUTN");//关注点在这
    var a=$('<a href="'+c.replace(/(\/\/\w+\.sinaimg\.cn\/)(\w+)(\/.+\.(gif|jpg|jpeg))/,"$1large$3")+'" target="_blank" class="view_img_link">[查看原图]</a>');
/*以下省略*/
/*以下省略*/
/*以下省略*/
}
```

从开头我们可以知道，e中存储了f获得的img-hash的文本，并同一个**复杂的字符串**传给了一个函数，该函数的返回直给了变量c，而一条似乎是在替换图片的链接。

因此进一步搜索函数名，有两个函数，选择上在`jandan_load_img()`函数上面一个

```js
var jdM4UbnT0aS3FCLFAblepJspDWH70CjuG6=function(m,r,d)
{
    var e="DECODE";
    var r=r?r:"";
    var d=d?d:0;
    var q=4;
    r=md5(r);
    var o=md5(r.substr(0,16));//substr() 方法可在字符串中抽取从 start（必须，后一个为length可选）下标开始的指定数目的字符。
    var n=md5(r.substr(16,16));
    if(q)
    {
        if(e=="DECODE")
            {
                var l=m.substr(0,q)
            }
    }
    else
    {
        var l=""
    }
    var c=o+md5(o+l);
    var k;
    if(e=="DECODE")
    {
        m=m.substr(q);
        k=base64_decode(m)
    }
    var h=new Array(256);
    for(var g=0;g<256;g++)
    {
        h[g]=g
    }
    var b=new Array();
    for(var g=0;g<256;g++)
    {
        b[g]=c.charCodeAt(g%c.length)//charCodeAt() 方法可返回指定位置的字符的 Unicode 编码。
    }
    for(var f=g=0;g<256;g++)
    {
        f=(f+h[g]+b[g])%256;
        tmp=h[g];
        h[g]=h[f];
        h[f]=tmp
    }
    var t="";
    k=k.split("");
    for(var p=f=g=0;g<k.length;g++)
    {
        p=(p+1)%256;
        f=(f+h[p])%256;
        tmp=h[p];
        h[p]=h[f];
        h[f]=tmp;
        t+=chr(ord(k[g])^(h[(h[p]+h[f])%256]))
    }
    if(e=="DECODE")
    {
        if((t.substr(0,10)==0||t.substr(0,10)-time()>0)&&t.substr(10,16)==md5(t.substr(26)+n).substr(0,16))
        {
            t=t.substr(26)
        }
        else
        {
            t=""
        }
    }
    return t
}
```

用python改写

```python
def image_url_decoder(m, r='', d=0):
    e = 'DECODE'
    q = 4
    r = md5_coder(r)
    o = md5_coder(r[0:16])
    n = md5_coder(r[16:31])

    l = m[0:q]

    c = o + md5_coder(o + l)
    m = m[q:]
    k = base64_preprocess(m)
    b = list(range(256))
    for g in range(256):
        b[g] = ord(c[g % len(c)])

    h = list(range(256))
    f = 0
    for g in range(256):
        f = (f + h[g] + b[g]) % 256
        h[g], h[f] = h[f], h[g]

    p, f = 0, 0
    t = ''
    for g in range(len(k)):
        p = (p + 1) % 256
        f = (f + h[p]) % 256
        h[p], h[f] = h[f], h[p]
        # 程序改来时移除ord，数组引用时，在python会直接产生整数而不是字符
        t += chr(k[g] ^ (h[(h[p] + h[f]) % 256]))

    img_url = 'http:'+ t[26:] # 测试时发现未加http:统一加后返回url
    return img_url
```

其中md5加密函数

```python
import hashlib
#吉大外网成绩查询时写过
def md5_coder(origin_text):
    m = hashlib.md5()
    m.update(origin_text.encode('utf-8'))
    cipher_text = m.hexdigest()

    return cipher_text
```

base64本来写的时候用的是`base64.decode()`后来测试时发现不行，查找原因后用b64decode，仍然报错，最后发现需要对位数进行补齐。

```python
# Python要求base64输入为4的整数倍，否则要用=补齐
import base64

def base64_preprocess(origin_text):
    missing_padding = 4 - len(origin_text) % 4
    if missing_padding:
        origin_text += '=' * missing_padding

    return base64.b64decode(origin_text)
```

融合后整个测试发现可以输出正确的url

#源代码

最后，加入链接爬虫，结合之前写的图片下载程序，完美运行

```python
import re
import time
import requests
import threading
import urllib.request
from bs4 import BeautifulSoup
from ImageUrlDecoder import image_url_decoder
from gevent.pool import Pool
from gevent import monkey

monkey.patch_all()# 需要在读写io时改变运算顺序，#不是很喜欢这样。。准备换个别的协程实现办法

'''种子url,headers'''
seed_url = 'http://jandan.net/ooxx/page-{}'
headers = {
    'Referer': 'http://jandan.net/ooxx',
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3386.1 Safari/537.36',
}

'''获得解密链接需要的key'''
def get_js_key(js_url):
    js_data = requests.get('http:'+js_url).text # 下载js文件
    js_key = re.findall('c=\w+\(e,"(.*?)"\)',js_data)[0] #匹配c=jdCcRGr2eNvcdQ8PSoxKHCF85TnihU52BH(e,"OerGLILvJX7DoY1dTrW50H62XpCEq3pf")
    return js_key

'''对一页进行分析''' 
#首先要获得网页数据
#其次获得图片的hash和解密的js密钥(每页都不同)
#最后解密出图片链接下载
def get_one_page_url(url,page):
    fout = open('failed_pages.txt', "a", encoding='utf-8')
    try:
        wb_data = requests.get(url,headers = headers)
    except:
        print(url, file=fout)  # 下载失败 保存链接
        print('Page: '+ str(page) + ' failed!')
    else:
        js_url = re.findall('<script src="//cdn.jandan.net/static/min/[\w]+\.[\d]+.js"></script>',wb_data.text)[-1]
        js_url = re.findall('(?<=").*?(?=\")',js_url)[0] #返回的是list
        js_key = get_js_key(js_url)
    
        soup = BeautifulSoup(wb_data.text,'lxml')
        img_hashes = soup.select('span.img-hash')
        one_page_img_url = []

        for each in img_hashes:
            img_hash = each.get_text()
            one_page_img_url.append(image_url_decoder(img_hash,js_key))
            #print(one_page_img_url)
        Coroutine_download(one_page_img_url) #下载

'''下载图片函数''' 
def download_pics(url):
    folder_path = 'D:/Python/JandanCrawler/Download_pics/'#保存失败链接
    fout = open('failed_pics.txt', "a", encoding='utf-8')
    try:
        urllib.request.urlretrieve(url, folder_path + url.split('/')[-1])
        time.sleep(1)
        print('pic: ' + url.split('/')[-1] + ' done!')
    except:
        print(url, file=fout)  # 下载失败 保存链接
        print('pic: ' + url.split('/')[-1] + ' failed!')

'''协程''' 
def Coroutine_download(urls):
    p = Pool()
    p.map(download_pics, urls)
    # g = [gevent.spawn(download_pics, url) for url in urls]
    # gevent.joinall(g)   

'''获得所有页面的url,并进行下载'''   
def get_all_page_url(start, end):
    for page in range(start, end):
        url = seed_url.format(page)
        time.sleep(1)
        get_one_page_url(url,page)

if __name__ == '__main__':
    t1 = time.time()
    get_all_page_url(1,65)
    print(time.time()-t1)
```

1700张图片 。。。在我的各种延时下，下了440秒。。我是知足了- -

<font size = 5> **相关文章:** </font>
<p>1. {% post_link 2018-1-15-crawler1 %}
2. {% post_link 2018-01-06-douban %}
3. {% post_link 2018-04-25-jlucjcx %}</p>

