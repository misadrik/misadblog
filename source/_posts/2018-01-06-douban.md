---
title: 学习笔记之爬虫(2):豆瓣电影top250
date: 2018-01-16 23:34:56
categories: 
- 技术
- 爬虫
tags: [爬虫,学习笔记,技术]
copyright: true
comments: true
reward: true
---

# 概述

学了两星期了，在此之前看的比较散，原理上太薄弱，所以想重新学一下，目前还是用BeautifulSoup和requests来做，很大一部分网站都能够顺利解析并获取想要的数据，试了小猪，58同城，现在自已写了豆瓣的爬虫，就当是对刚学知识的巩固，积累经验，并做下一部学习的准备。（就不说今天兴致勃勃爬微博，结果发现微博数据全是由js加载时那种想哭的冲动了，我学还不行嘛）
<!-- more -->
就按我写的思路贴代码吧

# 代码

先列出所有用到的库（当然这是在写的过程中慢慢加的）

```python
from bs4 import BeautifulSoup
import requests
import time
import csv
import re
from multiprocessing import Pool
```

初始设置的一些参数

```python
start_url = 'https://movie.douban.com/top250' # 初始url
# https://movie.douban.com/top250?start=0&filter= # 分析url组成
fieldnames = ['title', 'director','year','rate','desc','popu'] # csv表头 输入dict与输出csv对应关系
# 用于伪装成浏览器
headers = {
    'User-Agent':'',
    'Cookie':''
}
```

首先是要获取所有待爬取的链接，发现这是可以从页面底部的页码中获取的，不过没有当前页面，需要额外添加

```python
'''获取所有待爬取url函数'''
def get_all_links(url):
    wb_data = requests.get(url,headers = headers)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    links = soup.select('div.paginator > a')

    all_links = []
    all_links.append(start_url) # 没有这一条会错过top25
    
    for link in links:
        # print(link.get('href'))
        all_links.append(start_url + link.get('href')) #将相对地址变成绝对地址
    
    # print(all_links) #用于测试
    return all_links

```

然后是对单一页面进行分析，构建函数提取数据，这一过程是最麻烦，但也是最有趣的。用到的工具可以有很多就看基本功和智商了233

```python
'''获取单页信息函数'''
'''!important,实际使用过程中发现，有的电影没有description时，之后的desc会错位，并错过当页最后一个电影'''
'''目前没有太有效办法，想到的就是从上一级就获取数据，这样无非就是再复杂化筛选过程，不太经济，保留bug'''
def get_one_page_info(url):
    wb_data = requests.get(url,headers = headers)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    titles = soup.select('div.hd > a')
    directors = soup.select('div.bd > p:nth-of-type(1)')
    years = soup.select('div.bd > p:nth-of-type(1)')
    rates = soup.select('span.rating_num')
    descs = soup.select('div.bd > p.quote > span')
    popus = soup.select('div.bd > div > span:nth-of-type(4)')

    # print(names,directors,years,rates,descs)
    data_list = []
    for title,director,year,rate,desc,popu in zip(titles,directors,years,rates,descs,popus):
        # 调试用函数,可以对每一项先打印其字符串形式，然后再想办法选出数据    
        # print(repr(year.get_text())) # 以字符串样式打印，便于筛选数据
        # print(re.findall(r'\d+',year.get_text()))
        # print(desc.get_text().strip('\n'))
        # print("===") # 分隔
        # date = re.findall(r'\d+',year.get_text())
        time.sleep(3)
        data = {
            'title': title.get_text().strip('\n').replace('\n\xa0',' ').replace('\xa0',' '), # 替换掉无关字符
            'director': director.get_text().strip('\n').strip().split('\xa0\xa0\xa0')[0],
            'year': re.findall(r'\d+',year.get_text())[0],# 正则表达式，选出数字
            'rate': rate.get_text(),
            'desc': desc.get_text().strip('\n'),
            'popu': re.findall(r'\d+',popu.get_text())[0],
        }
        # print(data)
        data_list.append(data)
    return data_list
# get_one_page_info('https://movie.douban.com/top250?start=100&filter=') # 用于测试
```

由于数据规模小，重点是数据库没用熟，因此想到输出成csv文件，为了避免数据爬完后丢失，想办法爬一页写入一页，用追加方式打开，使光标位于文尾。

```python
'''写入csv文件函数'''
def dict_writer(dict_datas,fieldnames):
    with open('top250.csv', 'a',encoding = 'utf-8',newline = '') as f:
        # 'a' 以追加方式打开文件，encoding 则是因为有中文，newline防止输出额外空行
        # fieldnames = ['title', 'director','year','rate','desc']
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        #writer.writeheader() # 写表头
        for dict_data in dict_datas:
            writer.writerow(dict_data)
```

最后便是综合以上的，写出一个函数用于遍历并爬取所有的url，并后期测试了多进程爬取

```python
'''获取所有信息函数'''
def get_pages_info(url):
    all_links = get_all_links(url)
    
    data = []
    for link in all_links:
        data = get_one_page_info(link)
        dict_writer(data,fieldnames)
        print('One page done!') # 提示一页完成
        time.sleep(5)
    
    '''用于多进程测试'''
    # data = get_one_page_info(url)
    # dict_writer(data,fieldnames)
    # print('One page done!') # 提示一页完成
    # time.sleep(5)

if __name__ == '__main__':
    get_pages_info(start_url)
    '''用于多进程测试'''    
    # all_links = get_all_links(start_url)
    # pool = Pool() # 创建进程池
    # pool.map(get_pages_info,all_links)
```

多进程爬虫的测试是成功的（速度提高了4倍），但由于电影有排名先后，多进程时会导到输出到文件的排名会有问题（就是各页的顺序错乱），因此注释用于以后参考

# 存在的问题

1. **python基础知识薄弱**

   由于看了没多久，第一次做实用点的，以后多练习，并了解熟悉python的各种特性

2. **网页知识薄弱**

争取多练习吧。然后有空要补下正则，网页，数据库的知识，并熟悉目前使用的各种库，工作量还是很大的。

进阶的话。。。争取吧，目前还是巩固为主
