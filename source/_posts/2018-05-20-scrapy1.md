---
title: 学习笔记之爬虫(3):Scrapy之一
date: 2018-05-20 15:15:30
categories: 
- 技术
- 爬虫
tags: [爬虫,学习笔记,技术]
copyright: true
comments: true
reward: true
---

# 概述

先用`scrapy`粗略的举个例子，并用豆瓣电影top250作例子试了下。
1. 首先实现了用scrapy爬取豆瓣电影top250
2. 准备进一步更新对数据的处理

# 关于Scrapy
<div align=center>
![180520scrapy](http://wx1.sinaimg.cn/large/711dab24gy1frhtx9ieq4j20fa0asta7.jpg)
</div>
<!-- more -->

Scrapy主要包括了以下组件：

- **引擎(Scrapy)**
  用来处理整个系统的数据流处理, 触发事务(框架核心)
- **调度器(Scheduler)**
  用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址
- **下载器(Downloader)**
  用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)
- **爬虫(Spiders)**
  爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面
- **项目管道(Pipeline)**
  负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
- **下载器中间件(Downloader Middlewares)**
  位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
- **爬虫中间件(Spider Middlewares)**
  介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。
- **调度中间件(Scheduler Middewares)**
  介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。

Scrapy运行流程大概如下：

1. 引擎从调度器中取出一个链接(URL)用于接下来的抓取
2. 引擎把URL封装成一个请求(Request)传给下载器
3. 下载器把资源下载下来，并封装成应答包(Response)
4. 爬虫解析Response
5. 解析出实体（Item）,则交给实体管道进行进一步的处理
6. 解析出的是链接（URL）,则把URL交给调度器等待抓取

# 创建项目

`scrapy startproject ProjectName`

```python
scrapy startproject Doubancrawler
```

# 编辑items

items中为网页中提取的信息，可以根据需要自定义。

```python
import scrapy

class DoubancrawlerItem(scrapy.Item):
    # define the fields for your item here like:
    name = scrapy.Field()
    rates = scrapy.Field()
```
# 编辑crawler.py
在spiders下新建一个`doubancrawler.py`然后如下
```python
import scrapy
from doubancrawler.items import DoubancrawlerItem

class Myspider(scrapy.Spider):
    name = 'doubancrawler'
    start_urls = ['https://movie.douban.com/top250',]

    def parse(self, response):#会自动访问starturls中的链接
        item = DoubancrawlerItem()
        for movie in response.css('div.item'):
            # item['name'] = movie.css('div.info div.hd span.title::text').extract_first()
            item['name'] = movie.xpath('.//span[@class = "title"]/text()').extract_first()
            item['rates'] = movie.xpath(
                './/span[@class = "rating_num"]/text()').extract_first()
            
            yield item
        # <span class="next"><link rel="next" href="?start=25&amp;filter="/>
        next_page = response.css('span.next link::attr(href)').extract_first()
        if next_page is not None:
            yield response.follow(next_page,callback = self.parse)

```

其中用到了`xpath`和`css`选择器

然后在命令行中输入

`scrapy crawl Doubancrawler -o items.json`

发现没有返回结果，此时查看运行过程的提示发现是返回`code:403`，因此可能需要加上`headers`

打开`settings.py`在`DEFAULT_REQUEST_HEADERS`下加入`headers`，运行发现输出结果中文显示为Unicode编码，进一步修改scrapy的输出编码方式。

再次在`settings.py`中加入`FEED_EXPORT_ENCODING = 'utf-8'`

最终输出的是以字典形式存储的数据。

<font size = 5> **相关文章:** </font>
<p>1.  {% post_link 2018-01-06-douban %} </p>