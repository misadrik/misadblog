---
title: 学习笔记之爬虫(1):异步加载数据的爬取
date: 2018-01-15 23:34:55
categories: 
- 技术
- 爬虫
tags: [爬虫,学习笔记,技术]
copyright: true
comments: true
reward: true
---

# 概述

经过两天的学习终于完成了第一个稍微实用的爬虫，用15秒爬下了50张（哭）霉霉的图片。。。感谢仁慈的网站网开一面，由于学艺不精，暂时没加反反爬手段，只爬了前三页用于测试。实际测试已经成功，所以顺便复习下这几天学习的成果。
<!-- more -->
# 网页信息的的获取

目前所学习的还只是通过requests来得到网页，并用BeautifulSoup调取lxml html解析器来获取网页数据 （还有效率更高的正则表达式及lxml模块）。
```python
from bs4 import BeautifulSoup
import requests

url =''
header = {
    'User-Agent':''
}
# 'User-Agent'用于伪装成浏览器正常访问

wb_data = requests.get(url,headers = header)
soup = BeautifulSoup(wb_data.text,'lxml')
# .text是用于获取网页的文本类型数据
#'lxml'即调用lxml html解析器
```
# 网页元素的筛选

BeautifulSoup支持CSSselector，因此可使用***.select()***,技术方法用CSS选择器的语法找到存有信息的标签
目前有两种方法来选择***.select()***的参数（要具有唯一性特征）

* **CSSselector** 
  直接在Chrome检查元素后*copy -> selector*，然后选择到需要元素的父标签。
  这里遇到的问题是需要把*nth:child*改成*nth-of-type*然后就查了下这两个的区别，相比较*nth-of-type*条件更少。

| 选择器         | 条件                                |
| ----------- | --------------------------------- |
| nth-child   | 这是个段落元素且它父标签的第二个孩子元素(***\<p\>***标签) |
| nth-of-type | 父标签的第二个段落子元素(***\<p\>***标签)         |

但后来由于要选取所有的同类标签，所以并没有在这个问题上多作纠结。

* **标签名** 
  是当标签名唯一时，可以使用标签名的形式来进行选择数据

# 获取数据的打包

一个网页通过上述步骤，通常能获取许多不同链接，然后便通过循环来获得不同的数据，对于多个数据的循环用到了zip函数。
```python
# 用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同。
zip([iterable, ...])
zip(*zipped_list) #逆操作
```

然后是获取数据，一般分为两类，对于标题，分类等文本信息，使用***.get_text()***方法获取，而对于图片或链接等则用***.get('link')***获取，link为链接所在的属性，如src,href等。这其中涉及到的list和dict的操作不多说。在获取数据后再进一步根据下一步操作选择存储信息的方式，存于list,dict等

# 多页信息的爬取

目前主要涉及静态网页，通过观察链接的变化，设置循环来对多页网页进行爬取，此时由于访问过于频繁，可能会触发网站的反爬虫机制，一个简单的办法是通过***time.sleep()***方法来设置访问间隔

```python
import time

time.sleep(2) # 程序停止2秒
```

# 动态加载数据的提取

现在很多网站用动态加载，如新浪微博的评论，可以随鼠标的点击滚轮的运动，不停加载内容。这一过程是通过JS来完成的。
对于这类网页的爬取，需要用浏览器的监视功能中（***NETWORK -> XHR***)，在网页动态加载的过程中，查看加载的规律，对于内容的提取也应该是这样，***检查***加载的内容，然后分析网页中多出的html代码进而执行与之前相同的爬取步骤。

# 网页的下载

```python
import urllib.request
urllib.request.urlretrieve(url, filename=None, reporthook=None, data=None)
```

* 参数*filename*指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）
* 参数*reporthook*是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。
* 参数*data*指post导服务器的数据，该方法返回一个包含两个元素的(*filename*, *headers*) 元组，*filename* 表示保存到本地的路径，*header*表示服务器的响应头

在爬取霉霉图片时定义图片文件名时还用了split方法

```python
# str.split(str="", num=string.count(str)).
# str设定的分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。
# num 表示分割次数。
>>>str = 'misad is'
>>>print (str.split('i',1))
['m','sad is']
```

# Show me the code

```python
# 动态数据提取
from bs4 import BeautifulSoup
import requests, urllib.request
import time

url = 'https://weheartit.com/inspirations/taylorswift?scrolling=true&page='
header = {
    'User-Agent': '',
    }

def get_page(url, data = None):
    wb_data = requests.get(url,headers = header)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    imgs = soup.select('img.entry-thumbnail')

    one_page_img_url = []
    
    for img in imgs:
        one_page_img_url.append(img.get('src') )

    print(one_page_img_url)

    return one_page_img_url


def get_more_pages(start,end):
    imgs_url = []

    for one in range(start,end):
        one_page_img_url = get_page(url+str(one))
        time.sleep(4)

        imgs_url.extend(one_page_img_url)

    print(imgs_url)

    return imgs_url

def download_links(links):
    folder_path = ''
    for link in links:
        urllib.request.urlretrieve(link, folder_path + link.split('/')[-2] + link.split('/')[-1])


imgs_url = get_more_pages(1,3)
download_links(imgs_url)

```

